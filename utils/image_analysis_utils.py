# utils/image_analysis_utils.py

import torch
import torch.nn.functional as F
from PIL import Image
from functools import lru_cache
from transformers import CLIPProcessor, CLIPModel

from utils.image_utils import (
    get_transform_vit,
    load_model_vit,
    preprocess_image_vit
)

# Zero-shot CLIP setup (cached to avoid reload)
@lru_cache(maxsize=1)
def load_clip():
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    proc  = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    return model, proc

# Candidate prompts
SOURCE_CANDIDATES = [
    "image generated by MidJourney",
    "image generated by DALL·E",
    "image generated by Stable Diffusion",
    "image generated by AI"
]

def analyze_image(image: Image.Image, model_path: str = "models/vit_deepfake_detector_augmented.pth"):
    """
    Performs deepfake classification + zero-shot reverse trace.
    Returns:
      {
        "label": "REAL"|"DEEPFAKE",
        "confidence": float (0–1),
        "reverse_scores": [ {"label":..., "score":...}, ... ]  # only for DEEPFAKE
      }
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1) Classification
    model = load_model_vit(model_path).to(device)
    transform = get_transform_vit()
    tensor = preprocess_image_vit(image, transform).to(device)

    with torch.no_grad():
        logits = model(tensor)
        probs  = F.softmax(logits[0], dim=0)
        conf, idx = torch.max(probs, dim=0)
    # Note: assume class 0→DEEPFAKE, 1→REAL (adjust if needed)
    classes = ["DEEPFAKE", "REAL"]
    label = classes[idx.item()]

    result = {
        "label": label,
        "confidence": conf.item(),
        "reverse_scores": []
    }

    # 2) Reverse Traceability if flagged DEEPFAKE
    if label == "DEEPFAKE":
        clip_model, clip_proc = load_clip()
        inputs = clip_proc(text=SOURCE_CANDIDATES, images=image, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = clip_model(**inputs)
            clip_probs = outputs.logits_per_image.softmax(dim=1).squeeze()
        scores = [
            {"label": lbl, "score": float(clip_probs[i].item())}
            for i, lbl in enumerate(SOURCE_CANDIDATES)
        ]
        # sort descending
        result["reverse_scores"] = sorted(scores, key=lambda x: x["score"], reverse=True)

    return result
